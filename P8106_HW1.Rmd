---
title: "P8106_HW1"
author: "Ziyi Zhao"
date: "2/24/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(summarytools)
library(stargazer)
library(leaps)
library(caret)
library(FNN)
library(ModelMetrics)
library(Rcpp)
library(microbenchmark)
library(glmnet)
library(corrplot)
library(plotmo)
library(pls)

```


# Part a: Fit a linear model using least squares on the training data and calculate the mean square error using the test data.

We first look at the summary statistics of predictors and the response.

```{r}
sol_train = read_csv("./solubility_train.csv") %>% janitor::clean_names()
sol_test = read_csv("./solubility_test.csv") %>% janitor::clean_names()

st_options(plain.ascii = FALSE,
           style = "rmarkdown",
           dfSummary.silent = TRUE,
           footnote = NA,
           subtitle.emphasis = FALSE)

dfSummary(sol_train[,-229])
dfSummary(sol_test[,-229])

sol_all <- rbind(sol_train,sol_test)
dfSummary(sol_all[,-229])

```

We fit linear least square model to train dataset and calculate MSE using test dataset.

```{r}
linear_fit <- lm(solubility~.,data=sol_train)

linear_pred <- predict(linear_fit,sol_test)

mse(sol_test$solubility,linear_pred)

# KNN
mse_knn <- rep(0,19)
for (i in 2:20) {
  pred_knn <- knn.reg(train = sol_train[,1:228], test = sol_test[,1:228],
                     y = sol_train$solubility,k=i)
  mse_knn[i-1] <- mse(sol_test$solubility,pred_knn$pred)
}

min(mse_knn)

knn_k <- c(2:20)
tibble(knn_k,mse_knn) %>% 
  ggplot(aes(x=knn_k,y=mse_knn))+
  geom_point()+
  labs(title = "MSE change by different k",
       x = "k",
       y = "mse")


```


```{r}
set.seed(7)
lm.fit <- train(predictor_mtx,response_vtr,
                method = "lm",
                trControl = ctrl1)

pred.lm <- predict(lm.fit$finalModel,
                   newdata = data.frame(predictor_mtx_test))

mse(sol_test$solubility,pred.lm)

(lm.info <- postResample(predict(ridge.fit,predictor_mtx_test),
                         sol_test$solubility))
```




# Part b: Fit a ridge regression model on the training data, with λ chosen by cross-validation. Report the test error

Creat a correlation plot on total datasets (training + test) to see how predictors are correlated

```{r fig.width=25,fig.height=25}
predictor_mtx <- model.matrix(solubility~.,sol_all)[,-1]

response_vtr <- sol_all$solubility

corrplot(cor(predictor_mtx))

```

From the correlation plot, we can observe that there are some positively correlated predictors, shown as dark-blue dots in the plot.

Let's fit a ridge regression model on the training data, with lambda chosen by cross-validation

```{r fig.height=20,fig.width=25}
predictor_mtx <- model.matrix(solubility~.,sol_train)[,-1]
response_vtr <- sol_train$solubility

ridge.mod <- glmnet(predictor_mtx,response_vtr,standardize = TRUE,
                    alpha = 0,
                    lambda = exp(seq(-5,8,length=100)))
mat.coef <- coef(ridge.mod)
dim(mat.coef)

# trace plot
plot_glmnet(ridge.mod,xvar = "rlambda",label = 228)

```

The trace plot show how coefficients change as log lambda decreases.

Use cross validation to determine the optimal value of lambda. Then, use the best lambda to fit the ridge regression to test dataset and get MSE.

```{r}
set.seed(7)
cv.ridge <- cv.glmnet(predictor_mtx,response_vtr,type.measure = "mse",
                      alpha=0,lambda = exp(seq(-5,1,length=100)))
plot(cv.ridge)

best.lambda <- cv.ridge$lambda.1se
best.lambda

# coefficients of the final model
predict(ridge.mod,s=best.lambda,type = "coefficients")


predictor_mtx_test <- model.matrix(solubility~.,sol_test)[,-1]
ridge_pred <- predict(ridge.mod,s=best.lambda,newx = predictor_mtx_test)
mse(sol_test$solubility,ridge_pred[,1])


```

The optimal lambda acquired from cross-validation is `r best.lambda`.

The mean squared error we got is `r mse(sol_test$solubility,ridge_pred[,1])`.

Use caret to re-do ridge to see change of MSE.

```{r}
ctrl1 <- trainControl(method = "repeatedcv",number = 10,repeats = 5)

set.seed(7)
ridge.fit <- train(predictor_mtx,response_vtr,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha=0,
                                        lambda=exp(seq(-5,1,length=100))),
                   trControl = ctrl1)

plot(ridge.fit,xTrans = function(predictor_mtx)log(predictor_mtx))

ridge.fit$bestTune
coef(ridge.fit$finalModel,ridge.fit$bestTune$lambda)

pred.ridge <- predict(ridge.fit,s=ridge.fit$bestTune$lambda,
                      newdata = predictor_mtx_test)

mse(sol_test$solubility,pred.ridge)

(ridge_info <- postResample(predict(ridge.fit,predictor_mtx_test),
                           sol_test$solubility))

```

The mean square error of fitting ridge using carot is `r mse(sol_test$solubility,pred.ridge)`.

# Fit a lasso model on the training data, with λ chosen by cross-validation. Report the test error, along with the number of non-zero coefficient estimates.

```{r}
set.seed(7)
cv.lasso <- cv.glmnet(predictor_mtx,response_vtr,alpha=1,
                      lambda = exp(seq(-10,-1,length=100)))
plot(cv.lasso)

# trace plot
plot_glmnet(cv.lasso$glmnet.fit)

# coefficients of final model
predict(cv.lasso,s="lambda.1se",type="coefficients")

lasso_pred <- predict(cv.lasso,s="lambda.1se",newx = predictor_mtx_test)
mse(sol_test$solubility,lasso_pred[,1])

```

The mean squared error is `r mse(sol_test$solubility,lasso_pred[,1])`.

We use caret to redo lasso to see change of MSE.

```{r}
set.seed(7)
lasso.fit <- train(predictor_mtx,response_vtr,
                   method="glmnet",
                   tuneGrid = expand.grid(alpha=1,
                                          lambda=exp(seq(-10,1,length=100))),
                   trControl = ctrl1)
plot(lasso.fit,xTrans = function(predictor_mtx)log(predictor_mtx))

lasso.fit$bestTune
coef(lasso.fit$finalModel,lasso.fit$bestTune$lambda)

pred.lasso <- predict(lasso.fit,s=lasso.fit$bestTune$lambda,
                      newdata = predictor_mtx_test)

mse(sol_test$solubility,pred.lasso)

(lasso.info <- postResample(predict(lasso.fit,predictor_mtx_test),
                            sol_test$solubility))

```

The mean square error of fitting lasso using caret is `r mse(sol_test$solubility,pred.lasso)`.

# Fit a principle component regression model on the training data, with M chosen by cross-validation. Report the test error, along with the value of M selected by cross-validation.

```{r}
set.seed(7)
pcr.mod <- pcr(solubility~.,
               data=sol_train,
               scale=TRUE,
               validation="CV") 
summary(pcr.mod)

validationplot(pcr.mod,val.type = "MSEP",legendpos="topright")

cv.mse <- RMSEP(pcr.mod)
ncomp.cv <- which.min(cv.mse$val[1,,])-1
ncomp.cv

pcr_pred <- predict(pcr.mod,newdata = predictor_mtx_test,
                    ncomp = ncomp.cv)
mse(sol_test$solubility,pcr_pred)

```

The mean square error of fitting PCR is `r mse(sol_test$solubility,pcr_pred)`.

```{r}
set.seed(7)
pcr.fit <- train(predictor_mtx,response_vtr,
                 method = "pcr",
                 tuneGrid = data.frame(1:228),
                 trControl = ctrl1,
                 preProc=c("center","scale"))

trans <- preProcess(predictor_mtx,method = c("center","scale"))
pred.pcr <- predict(pcr.fit$finalModel,newdata = predict(trans,predictor_mtx_test),
                    ncomp = pcr.fit$bestTune$ncomp)

mse(sol_test$solubility,pred.pcr)

(pcr.info <- postResample(predict(pcr.fit,predictor_mtx_test),
                          sol_test$solubility))

```

The mean square error of fitting PCR using caret is `r mse(sol_test$solubility,pred.pcr)`.

# Brieﬂy discuss the results obtained in (a)∼(d).

```{r}
resamp <- resamples(list(lm = lm.fit,
                         ridge = ridge.fit,
                         lasso = lasso.fit,
                         pcr = pcr.fit))
summary(resamp)

```





