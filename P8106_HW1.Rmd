---
title: "P8106_HW1"
author: "Ziyi Zhao"
date: "2/24/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(summarytools)
library(stargazer)
library(leaps)
library(caret)
library(FNN)
library(ModelMetrics)
library(Rcpp)
library(microbenchmark)
library(glmnet)
library(corrplot)
library(plotmo)

```


# Part a: Fit a linear model using least squares on the training data and calculate the mean square error using the test data.

We first look at the summary statistics of predictors and the response.

```{r}
sol_train = read_csv("./solubility_train.csv") %>% janitor::clean_names()
sol_test = read_csv("./solubility_test.csv") %>% janitor::clean_names()

st_options(plain.ascii = FALSE,
           style = "rmarkdown",
           dfSummary.silent = TRUE,
           footnote = NA,
           subtitle.emphasis = FALSE)

dfSummary(sol_train[,-229])
dfSummary(sol_test[,-229])

sol_all <- rbind(sol_train,sol_test)
dfSummary(sol_all[,-229])

```

We fit linear least square model to train dataset and calculate MSE using test dataset.

```{r}
linear_fit <- lm(solubility~.,data=sol_train)

linear_pred <- predict(linear_fit,sol_test)

mse(sol_test$solubility,linear_pred)

# KNN
mse_knn <- rep(0,19)
for (i in 2:20) {
  pred_knn <- knn.reg(train = sol_train[,1:228], test = sol_test[,1:228],
                     y = sol_train$solubility,k=i)
  mse_knn[i-1] <- mse(sol_test$solubility,pred_knn$pred)
}

min(mse_knn)

knn_k <- c(2:20)
tibble(knn_k,mse_knn) %>% 
  ggplot(aes(x=knn_k,y=mse_knn))+
  geom_point()+
  labs(title = "MSE change by different k",
       x = "k",
       y = "mse")


```


# Part b: Fit a ridge regression model on the training data, with λ chosen by cross-validation. Report the test error

Creat a correlation plot on total datasets (training + test) to see how predictors are correlated

```{r fig.width=25,fig.height=25}
predictor_mtx <- model.matrix(solubility~.,sol_all)[,-1]

response_vtr <- sol_all$solubility

corrplot(cor(predictor_mtx))

```

From the correlation plot, we can observe that there are some positively correlated predictors, shown as dark-blue dots in the plot.

Let's fit a ridge regression model on the training data, with lambda chosen by cross-validation

```{r fig.height=20,fig.width=25}
predictor_mtx <- model.matrix(solubility~.,sol_train)[,-1]
response_vtr <- sol_train$solubility

ridge.mod <- glmnet(predictor_mtx,response_vtr,standardize = TRUE,
                    alpha = 0,
                    lambda = exp(seq(-5,8,length=100)))
mat.coef <- coef(ridge.mod)
dim(mat.coef)

# trace plot
plot_glmnet(ridge.mod,xvar = "rlambda",label = 228)

```

The trace plot show how coefficients change as log lambda decreases.

Use cross validation to determine the optimal value of lambda. Then, use the best lambda to fit the ridge regression to test dataset and get MSE.

```{r}
set.seed(7)
cv.ridge <- cv.glmnet(predictor_mtx,response_vtr,type.measure = "mse",
                      alpha=0,lambda = exp(seq(-5,1,length=100)))
plot(cv.ridge)

best.lambda <- cv.ridge$lambda.min
best.lambda

# coefficients of the final model
predict(ridge.mod,s=best.lambda,type = "coefficients")


predictor_mtx_test <- model.matrix(solubility~.,sol_test)[,-1]
ridge_pred <- predict(ridge.mod,s=best.lambda,newx = predictor_mtx_test)
mse(sol_test$solubility,ridge_pred[,1])


```

The optimal lambda acquired from cross-validation is `r best.lambda`.

The mean squared error we got is `r mse(sol_test$solubility,ridge_pred[,1])`.

# Fit a lasso model on the training data, with λ chosen by cross-validation. Report the test error, along with the number of non-zero coeﬃcient estimates.

```{r}





```



